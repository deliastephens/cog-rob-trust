{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd82ca2",
   "metadata": {},
   "source": [
    "# Mini problem set: Trust Modeling in Human-Robot interactions.\n",
    "## Due Wednesday, April 25, 11:59 p.m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032e4a9",
   "metadata": {},
   "source": [
    "Make sure you load the dependencies below by highlighting the cell below and pressing Shift + Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2feee49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "from utils import check_omega, test_ok, check_explicability_score, check_matrix, check_cost, check_MDP, prep_browser, run_pyperplan, run_and_viz_pyperplan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c653c3",
   "metadata": {},
   "source": [
    "As discussed in lecture, we know that _explaining_ unexpected behaviors from our robot can increase trust. In this module, we'll implement the Meta-MDP solution to the Trust-Aware Planning problem from the simple Wumpus World example problem.\n",
    "\n",
    "## Wumpus World Modeling\n",
    "\n",
    "Because we're so familiar with PDDL, we'll model our Wumpus World as a PDDL problem. **There's no need for you to implement anything here; this code is just so you remember what a plan looks like!**\n",
    "\n",
    "Here, our robot is trying to navigate in a 3x3 grid to pick up a block. The robot knows that its shortest path is unencumbered, but the human has no idea--they think that there's a pile of trash on the shortest path to the green dot.\n",
    "\n",
    "![Wumpus World](res/wumpus-world.png)\n",
    "\n",
    "We've provided you with a fully modeled version of the robot world in `pddl/robot-domain.pddl` and `pddl/robot-problem.pddl`. Notice that none of our actions are `durative-actions`; we assume a unit cost for every action.\n",
    "\n",
    "Instead of using Optic to run our plans, we'll use a Python PDDL Planner called [`pyperplan`](https://github.com/aibasel/pyperplan). Pyperplan is cool because it is extensible and contains a very clean implementation of some of the commmon search algorithms; if you're interested in the way planners work, definitely check out their codebase! Like Optic, however, Pyperplan only supports _positive preconditions._  \n",
    "\n",
    "Let's compare our robot plan to our human plan. Run the following two cells to see what the human thinks the robot should do vs. what the robot thinks is optimal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7adc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "      <strong>Plan found!</strong>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"timeline-graph-1\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script> window.display_timeline(\"timeline-graph-1\", \"0.0: (robot-move robot1 sq2-2 sq1-2) [1.0]\\n1.0: (robot-move robot1 sq1-2 sq0-2) [1.0]\\n\") </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>raw planner output:</strong>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(robot-move robot1 sq2-2 sq1-2)\n",
      "(robot-move robot1 sq1-2 sq0-2)\n"
     ]
    }
   ],
   "source": [
    "domain_file = 'pddl/robot-domain.pddl'\n",
    "problem_file = 'pddl/robot-problem.pddl'\n",
    "\n",
    "run_and_viz_pyperplan(domain_file, problem_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3d5292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "      <strong>Plan found!</strong>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"timeline-graph-2\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script> window.display_timeline(\"timeline-graph-2\", \"0.0: (robot-move robot1 sq2-2 sq2-1) [1.0]\\n1.0: (robot-move robot1 sq2-1 sq2-0) [1.0]\\n2.0: (robot-move robot1 sq2-0 sq1-0) [1.0]\\n3.0: (robot-move robot1 sq1-0 sq0-0) [1.0]\\n4.0: (robot-move robot1 sq0-0 sq0-1) [1.0]\\n5.0: (robot-move robot1 sq0-1 sq0-2) [1.0]\\n\") </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>raw planner output:</strong>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(robot-move robot1 sq2-2 sq2-1)\n",
      "(robot-move robot1 sq2-1 sq2-0)\n",
      "(robot-move robot1 sq2-0 sq1-0)\n",
      "(robot-move robot1 sq1-0 sq0-0)\n",
      "(robot-move robot1 sq0-0 sq0-1)\n",
      "(robot-move robot1 sq0-1 sq0-2)\n"
     ]
    }
   ],
   "source": [
    "domain_file = 'pddl/human-domain.pddl'\n",
    "problem_file = 'pddl/human-problem.pddl'\n",
    "\n",
    "run_and_viz_pyperplan(domain_file, problem_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012c0ad",
   "metadata": {},
   "source": [
    "# The Meta-MDP\n",
    "\n",
    "This is where the real fun begins! As discussed in class, one way of selecting robot behaviors can be modeled as a \"Meta-MDP\". We'll use the [Python MDP Toolbox](https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html). \n",
    "\n",
    "Recall our human-aware planning problem from lecture (and implemented above):\n",
    "\n",
    "## Human-Aware Planning\n",
    "Recall the Human-Aware Planning problem described in [1](https://arxiv.org/pdf/2105.01220.pdf).\n",
    "\n",
    "**Input:**\n",
    "\n",
    "$\\mathcal{M}^R$, the robot's model of the environment and problem. Consists of the tuple $\\langle\\mathcal{D}^R, \\mathcal{I}^R, \\mathcal{G}^R\\rangle$, where $\\mathcal{D}^R$ is the domain,  $\\mathcal{I}^R$ is the initial state, and $\\mathcal{G}^R$ is the goal state.\n",
    "\n",
    "$\\mathcal{M}^G$, the human's model of the environment and problem. Consists of the tuple $\\langle\\mathcal{D}^H, \\mathcal{I}^H, \\mathcal{G}^H\\rangle$, where $\\mathcal{D}^H$ is the domain,  $\\mathcal{I}^H$ is the initial state, and $\\mathcal{G}^H$ is the goal state.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "A _plan_; that is, a sequence of robot actions that achieve the goal state but also meets the human's expectations. We call the degree to which the robot plan $\\pi$ matches the human expectations $\\pi^e$ the plan _explicability_, and we often model it as the _distance_ $\\delta$ between $\\pi^e$ and $\\pi$: \n",
    "$$\n",
    "E(\\pi) = -1 * \\delta(\\pi^e, \\pi)\n",
    "$$\n",
    "\n",
    "A plan $\\pi$ is _perfectly explicable_ if $E(\\pi) = 0$. We often use the difference in costs between the two plans as our distance function, $\\delta$.\n",
    "\n",
    "### Problem 1: Explicability\n",
    "\n",
    "Let's create a function to determine the explicability score which is the negative of the cost difference between the current plan and the optimal plan in the robot model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d3776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicability_score(optimal_cost, expected_cost):\n",
    "    \"\"\"\n",
    "    Builds the explicability score E(pi). Recall that E(plan) = - (plan_cost -\n",
    "    optimal_plan)\n",
    "\n",
    "    @param  optimal_cost:   The cost of the optimal plan.\n",
    "    @param  expected_cost:  The cost of the fully explicable plan.\n",
    "\n",
    "    @return E:             The explicability score given the optimal and explicable plans.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4403be8",
   "metadata": {},
   "source": [
    "Now, let's check the function you wrote!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_explicability_score(explicability_score)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61dd582",
   "metadata": {},
   "source": [
    "### Problem 2: It's MDPs All the Way Down\n",
    "As in the problem statement, we'll model the problem as an infinite horizon discounted MDP of the form $$M = \\langle S, A, P, C, \\gamma \\rangle$$\n",
    "\n",
    "#### Problem Description\n",
    "Our **state space**, $S$, are the human's \"trust level.\" For this implementation, we'll have four trust levels, so $\\| S \\| = 4$. We associate each trust level with numerical values $T = \\begin{bmatrix} 0 & 0.3 & 0.6 & 1.0 \\end{bmatrix}$, which we will use to help us model the rest of the problem.\n",
    "\n",
    "Our **action space**, $A$, is simple--the robot may choose between its own _optimal plan_ or the human's fully explainable (or _expected_ plan). (Therefore, $\\|A \\| = 2$, and $A(0) = \\pi^\\textrm{opt}$ and $A(1) = \\pi^\\textrm{exp}$). \n",
    "\n",
    "The **explicability score**, $E(\\pi)$, is the negative of the cost difference between the current plan and the optimal plan in the robot model. For example, $E(\\pi^\\textrm{exp}) = - (\\textrm{cost}_{\\pi^{\\textrm{exp}}} - \\textrm{cost}_{\\pi^{\\textrm{opt}}})$.\n",
    "\n",
    "As described in the [Python MDP Toolbox documentation](https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html), our transition matrix $P$ should be a [numpy array](https://numpy.org/doc/stable/reference/generated/numpy.array.html) of size `(2, 4, 4)`. Therefore, `P[k][i][j]` represents the likelihood of transitioning from state $s_i$ to state $s_j$ with the action $a_k$. There are two cases to think about when defining our transition matrix:\n",
    "\n",
    "1. The **optimal plan**. Here, we need to consider three subcases, as our robot is following a plan with a non-perfect explicability score. The trust level may either **decrease**, **stay the same**, or **increase**. \n",
    "   - We model the likelihood that the trust level **decreases** as $P(s_i, a^\\pi, s_{i-1}) = \\omega(i) * (1 - E(\\pi))$. \n",
    "   - We model the likelihood that the trust level **stays the same** as $P(s_i, a^\\pi, s_{i}) = \\omega(i) * E(\\pi)$.\n",
    "   - We model the likelihood that the trust level **increases** as $P(s_i, a^\\pi, s_{i+1}) = (1 - \\omega(i))$\n",
    "2.  The **expected plan**. Here, trust increases to the next level in all but the maximum trust level (where it is expected to remain the same).\n",
    "\n",
    "Note that `P(0)` corresponds with the transition matrix of the optimal plan, and `P(1)` corresponds with the transition matrix of the expected plan.\n",
    "\n",
    "\n",
    "The **cost**, $C$, is modeled as a `numpy array` of size `(4, 2)`. $C(s_i, a^\\pi) = (1 - \\omega(i)) * C_e(\\pi)$, where $C_e(\\pi)$ is the cost of the fully explainable plan. Here, we'll assume that each action has unit cost, but this cost function could certainly get more complicated!\n",
    "\n",
    "The likelihood that the human chooses to observe at some trust level, $\\omega(i)$ is modeled as a Bernoulli distribution with probability of $(1 - T(i))$. Here, $\\omega$ should be a `numpy array` of size `(1, 4)`.\n",
    "\n",
    "#### Problem Statement\n",
    "Your task is to write the functions `omega`, `transition_matrix`, and `cost` to model the MDP as described above. Once we're done modeling, we'll run policy iteration on our derived MDP and see what sort of results we get!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "553bec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omega(T):\n",
    "    \"\"\"\n",
    "    Builds the omega matrix. Recall that w(i) = (1 - T(i))\n",
    "\n",
    "    @param  T:     The numerical values for trust.\n",
    "    \n",
    "    @return w:     An np.array of np.shape(T). Recall that w(i)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_omega(omega)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e76e61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(T, w, E):\n",
    "    \"\"\"\n",
    "    Builds the transition matrix, P. Recall that we have 2 actions (size of E)\n",
    "    and 4 states (size of T). Therefore, our P matrix is an np.array of size\n",
    "    (length(E), length(T), length(T)).\n",
    "\n",
    "\n",
    "    The first action in P is following the optimal plan. The second action in P\n",
    "    is following the explainable plan. See the problem statement for a full\n",
    "    description of our expected transitions.\n",
    "    \n",
    "    @param  T: the trust levels\n",
    "    @param  w: the likelihood that the human observes\n",
    "    @param  E: the plan explicability score\n",
    "\n",
    "    @return P: the transition matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79758195",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_matrix(transition_matrix)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e72182c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w, E, expected_cost):\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95654f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cost(cost)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b82aa9",
   "metadata": {},
   "source": [
    "Now that we have these helper functions, let's build the Meta-MDP model! For this, use a simple cost function where each action has a unit cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11a8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def calculate_plan_cost(plan):\n",
    "    \"\"\"Calculates the cost of a plan. In our case (because each action has a unit cost), \n",
    "    the plan cost is simply the length of the plan. \"\"\"\n",
    "    return len(plan)\n",
    "\n",
    "def build_meta_MDP(T, optimal_plan, expected_plan):\n",
    "    \"\"\"\n",
    "    Builds the Meta MDP model given trust levels and an optimal and expected plan.\n",
    "\n",
    "    @param          T: The matrix associating trust level with values in the range [0, 1].\n",
    "    @param          optimal_plan: A list of actions corresponding with the optimal plan.\n",
    "    @param          expected_plan: A list of actions corresponding with the explicable/expected plan.\n",
    "\n",
    "    @return P:      The transition matrix.\n",
    "    @return C:      The cost matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the w (omega) matrix, representing the likelihood that a human will\n",
    "    # choose to observe\n",
    "    w = omega(T)\n",
    "    \n",
    "    # Get optimal and expected costs\n",
    "    optimal_cost = calculate_plan_cost(optimal_plan)\n",
    "    expected_cost = calculate_plan_cost(expected_plan)\n",
    "    \n",
    "    # Get E (explicability score)\n",
    "    E = explicability_score(optimal_cost, expected_cost)\n",
    "    \n",
    "    # Get P (transition matrix)\n",
    "    P = transition_matrix(T, w, E)\n",
    "    \n",
    "    # Get C (cost)\n",
    "    C = cost(w, E, expected_cost)\n",
    "    \n",
    "    return P, C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef91aa3",
   "metadata": {},
   "source": [
    "Finally, we'll try running the MDP that we wrote! First, let's create the optimal and expected plans given the examples, and then, execute the following cell to see your generated policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14421e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_domain_file = 'pddl/human-domain.pddl'\n",
    "human_problem_file = 'pddl/human-problem.pddl'\n",
    "\n",
    "robot_domain_file = 'pddl/robot-domain.pddl'\n",
    "robot_problem_file = 'pddl/robot-problem.pddl'\n",
    "\n",
    "expected_plan = run_pyperplan(human_domain_file, human_problem_file)\n",
    "optimal_plan = run_pyperplan(robot_domain_file, robot_problem_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d873edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.array([0, 0.3, 0.6, 1])\n",
    "P, C = build_meta_MDP(T, optimal_plan, expected_plan)\n",
    "gamma = 0.9\n",
    "\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, C, gamma)\n",
    "pi.run()\n",
    "\n",
    "pi.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f2b5c",
   "metadata": {},
   "source": [
    "Notice that your policy takes action $1$ (the explainable plan) in all cases except where human trust is very high. This is what we might expect intuitively, so that's pretty cool!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
