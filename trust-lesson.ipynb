{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust in Multi-Agent Systems\n",
    "Welcome to our lesson on trust and explainability in AI! \n",
    "\n",
    "# Explainability\n",
    "As discussed in lecture, we know that _explaining_ unexpected behaviors from our robot can increase trust. In this module, we'll see how this comes into play. Recall our human-aware planning problem from lecture:\n",
    "\n",
    "## Human-Aware Planning\n",
    "Recall the Human-Aware Planning problem described in [1](https://arxiv.org/pdf/2105.01220.pdf).\n",
    "\n",
    "**Input:**\n",
    "\n",
    "$\\mathcal{M}^R$, the robot's model of the environment and problem. Consists of the tuple $\\langle\\mathcal{D}^R, \\mathcal{I}^R, \\mathcal{G}^R\\rangle$, where $\\mathcal{D}^R$ is the domain,  $\\mathcal{I}^R$ is the initial state, and $\\mathcal{G}^R$ is the goal state.\n",
    "\n",
    "$\\mathcal{M}^G$, the human's model of the environment and problem. Consists of the tuple $\\langle\\mathcal{D}^R, \\mathcal{I}^R, \\mathcal{G}^R\\rangle$, where $\\mathcal{D}^R$ is the domain,  $\\mathcal{I}^R$ is the initial state, and $\\mathcal{G}^R$ is the goal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2efb303a8c67ecba5167d2efb1d6294a5dab7ce44c08b1fa1f624f44f77a78fb"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
