{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust in Multi-Agent Systems\n",
    "Welcome to our lesson on trust and explainability in AI! \n",
    "\n",
    "Before we get started, remember to run this cell to get everything set up correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import prep_browser, run_pyperplan, run_and_viz_pyperplan\n",
    "\n",
    "from pyperplan import task\n",
    "import re\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\nrequire(\"notebook/js/outputarea\").OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n\nvar tinycolor;\n\nrequire.config({\n  paths: {\n      sha1: 'http://editor.planning.domains/plugins/featured/timeline-viewer/sha1.js',\n      charts: 'https://www.gstatic.com/charts/loader.js'\n  }\n});\n\nrequirejs(['https://www.gstatic.com/charts/loader.js'],\n          function() {\n              google.charts.load('current', {packages: ['timeline']});\n         });\n\nrequirejs(['https://courses.csail.mit.edu/16.410/js/sha1.js'],\n          function() {\n              console.log('Loaded SHA1');\n         });\n\nrequirejs(['https://courses.csail.mit.edu/16.410/js/tinycolor.js'],\n          function(m) {\n              console.log('Loaded tinycolor');\n              tinycolor = m;\n         });\n\nfunction get_terms(pred) {\n  // Cut off the first and last characters - the parenthesis\n  pred = pred.trim().slice(1, pred.length - 1);\n  var words = pred.split(\" \");\n  return words;\n}\n\nfunction wordToColor(word, sat, lightness) {\n  // Helper function that converts a word to a color based on the SHA1 sum\n  var sha1_word = CryptoJS.SHA1(word).words[0];\n  sha1_word = sha1_word % 360;\n  if (sha1_word < 0) {\n    sha1_word += 360;\n  }\n  var hsl_string = 'hsl(' + sha1_word + \", \" + (100 * sat) + \"%, \" + (100 * lightness) + \"%)\";\n  return '#' + tinycolor(hsl_string).toHex();\n}\n\nfunction activity_to_table_row(match, id) {\n  var start_time = 1000*parseFloat(match[1]);\n  var end_time = 1000*parseFloat(match[3]) + start_time;\n  return [\"\" + id, match[2],  start_time, end_time];\n}\n\nfunction activity_to_color(match) {\n  var words = get_terms(match[2]);\n  var action_name = words[0].trim();\n  var color = wordToColor(action_name, 0.7, 0.75);\n  return color;\n}\n\nfunction display_timeline(chart_div, plan_string) {\n    requirejs(['https://www.gstatic.com/charts/loader.js'],\n        function() {\n            google.charts.load('current', {packages: ['timeline']});\n            google.charts.setOnLoadCallback(function () {display_timeline_cb(chart_div, plan_string)});\n        });\n}\n\nwindow.display_timeline = display_timeline;\n\nfunction display_timeline_cb(div_name, plan_string){\n    // Add the google chart\n    var options = {\n      height: 100,\n      animation: {\n        duration: 1000,\n        easing: 'out',\n      },\n      hAxis: {\n        gridlines: {count: 15}\n      },\n      timeline: {\n        showRowLabels: true,\n        groupByRowLabel: false,\n        colorByRowLabel: false,\n      }\n    };\n    var chart = new google.visualization.Timeline(document.getElementById(div_name));\n    \n    // Get planner text\n    var planner_output = plan_string;\n    // Find all matches for lines that appear to be temporal actions\n    var regexp = /^([\\d.]+)\\s*:\\s*(\\(.*\\))\\s*\\[([\\d.]+)\\]$/gm;\n    var matches = [];\n    var match;\n    while ((match = regexp.exec(planner_output)) !== null) {\n      matches.push(match);\n    }\n\n    // Load the data into a table\n    var data = new google.visualization.DataTable();\n    data.addColumn('string', 'Task ID');\n    data.addColumn('string', 'Task Name');\n    data.addColumn('number', 'Start');\n    data.addColumn('number', 'End');\n    data.addRows([]);\n    var colors_for_activities = [];\n    for(var i = 0; i < matches.length; i++) {\n        var match = matches[i];\n        data.addRows([activity_to_table_row(match, i)])\n        var color = activity_to_color(match)\n        colors_for_activities.push(color);\n    }\n\n    // Draw the chart!\n    options.height = data.getNumberOfRows() * 43 + 100;\n    options.colors = colors_for_activities;\n    chart.draw(data, options);\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prep_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability\n",
    "As discussed in lecture, we know that _explaining_ unexpected behaviors from our robot can increase trust. In this module, we'll see how this comes into play. Recall our human-aware planning problem from lecture:\n",
    "\n",
    "## Human-Aware Planning\n",
    "Recall the Human-Aware Planning problem described in [1](https://arxiv.org/pdf/2105.01220.pdf).\n",
    "\n",
    "**Input:**\n",
    "\n",
    "$\\mathcal{M}^R$, the robot's model of the environment and problem. Consists of the tuple $\\langle\\mathcal{D}^R, \\mathcal{I}^R, \\mathcal{G}^R\\rangle$, where $\\mathcal{D}^R$ is the domain,  $\\mathcal{I}^R$ is the initial state, and $\\mathcal{G}^R$ is the goal state.\n",
    "\n",
    "$\\mathcal{M}^G$, the human's model of the environment and problem. Consists of the tuple $\\langle\\mathcal{D}^H, \\mathcal{I}^H, \\mathcal{G}^H\\rangle$, where $\\mathcal{D}^H$ is the domain,  $\\mathcal{I}^H$ is the initial state, and $\\mathcal{G}^H$ is the goal state.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "A _plan_; that is, a sequence of robot actions that achieve the goal state but also meets the human's expectations. We call the degree to which the robot plan $\\pi$ matches the human expectations $\\pi^e$ the plan _explicability_, and we often model it as the _distance_ $\\delta$ between $\\pi^e$ and $\\pi$:\n",
    "\n",
    "$$\n",
    "E(\\pi) = -1 * \\delta(\\pi^e, \\pi)\n",
    "$$\n",
    "\n",
    "A plan $\\pi$ is _perfectly explicable_ if $E(\\pi) = 0$. We often use the difference in costs between the two plans as our distance function, $\\delta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Modeling Incomplete Knowledge\n",
    "Because we're so familiar with PDDL, we will model both of these problems in PDDL! Check out the references from `ps-03` if you've forgotten how to use it. **TODO: describe this stuff in more detail.**\n",
    "\n",
    "We'll start with a simplified version of the [Wumpus World](http://users.cecs.anu.edu.au/~patrik/pddlman/wumpus.html). Here, our robot is trying to navigate in a 3x3 grid to pick up a block. The robot knows that its shortest path is unencumbered, but the human has no idea--they think that there's piles of trash on the shortest path to the robot.\n",
    "\n",
    "![Wumpus World](res/wumpus-world.png)\n",
    "\n",
    "We've provided you with a fully modeled version of the robot world in `pddl/robot-domain.pddl` and `pddl/robot-problem.pddl`. Your task is to modify `pddl/human-domain.pddl` and `pddl/human-problem.pddl` so it reflects the fact that the _human_ thinks that there is trash in squares `(0, 2)` and `(1, 2)`. Notice that none of our actions are `durative-actions`; we assume a unit cost for every action.\n",
    "\n",
    "Instead of using Optic to run our plans, we'll use a Python PDDL Planner called [`pyperplan`](https://github.com/aibasel/pyperplan). Pyperplan is cool because it is extensible and contains a very clean implementation of some of the commmon search algorithms; if you're interested in the way planners work, definitely check out their codebase! Like Optic, however, Pyperplan only supports _positive preconditions._  \n",
    "\n",
    "Note, too, that we cannot specify negative initial conditions. This is important with our `clear` predicate. If we want to say that a square is not `clear` at the start, we simply omit it from the list of clear squares. See `robot-problem.pddl` for a concrete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Op (robot-move robot1 sq2-2 sq1-2)>, <Op (robot-move robot1 sq1-2 sq0-2)>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_file = 'pddl/robot-domain.pddl'\n",
    "problem_file = 'pddl/robot-problem.pddl'\n",
    "\n",
    "run_pyperplan(domain_file, problem_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "      <strong>Plan found!</strong>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"timeline-graph-1\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script> window.display_timeline(\"timeline-graph-1\", \"0.0: (robot-move robot1 sq2-2 sq1-2) [1.0]\\n1.0: (robot-move robot1 sq1-2 sq0-2) [1.0]\\n\") </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>raw planner output:</strong>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(robot-move robot1 sq2-2 sq1-2)\n",
      "(robot-move robot1 sq1-2 sq0-2)\n"
     ]
    }
   ],
   "source": [
    "run_and_viz_pyperplan(domain_file, problem_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we modify this domain to represent the human's belief that there is trash at position $(1, 2)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "      <strong>Plan found!</strong>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"timeline-graph-2\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script> window.display_timeline(\"timeline-graph-2\", \"0.0: (robot-move robot1 sq2-2 sq2-1) [1.0]\\n1.0: (robot-move robot1 sq2-1 sq2-0) [1.0]\\n2.0: (robot-move robot1 sq2-0 sq1-0) [1.0]\\n3.0: (robot-move robot1 sq1-0 sq0-0) [1.0]\\n4.0: (robot-move robot1 sq0-0 sq0-1) [1.0]\\n5.0: (robot-move robot1 sq0-1 sq0-2) [1.0]\\n\") </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>raw planner output:</strong>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(robot-move robot1 sq2-2 sq2-1)\n",
      "(robot-move robot1 sq2-1 sq2-0)\n",
      "(robot-move robot1 sq2-0 sq1-0)\n",
      "(robot-move robot1 sq1-0 sq0-0)\n",
      "(robot-move robot1 sq0-0 sq0-1)\n",
      "(robot-move robot1 sq0-1 sq0-2)\n"
     ]
    }
   ],
   "source": [
    "domain_file = 'pddl/human-domain.pddl'\n",
    "problem_file = 'pddl/human-problem.pddl'\n",
    "\n",
    "run_and_viz_pyperplan(domain_file, problem_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Plan Explainer\n",
    "\n",
    "If you were the human in this human-robot team, you'd probably be concerned if your robot started moving towards where you thought was a gigantic pile of trash! This corresponds to a _decrease in trust_. As discussed in lecture, there are several ways to deal with these trust decreases:\n",
    "\n",
    "**TODO: enumerate the ways trust can decrease**\n",
    "\n",
    "Today, we'll be focusing on _explaining_; that is, telling our robot it's probably a good idea to explain its actions when it takes unexpected actions. We'll accomplish this by modifying the PDDL plan with an `explain` action that has some cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try writing a function that will perform the robot's optimal plan, but explain to the human why their plans differ. We've implemented a few helper functions below to get you started, as well as outlined some pseudocode in the form of comments in the function `make_explainable_plan.` \n",
    "\n",
    "At a high-level, your function should take in as input an optimal robot plan and the expected human plan. It should then compare, action-by-action, the optimal action and the expected action, terminating when the optimal plan has reached the goal state. \n",
    "\n",
    "If the actions don't match up, you should check to see if the next optimal action is ever represented in the explainable plan. Then, you should skip to that action in the explainable plan and explain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_agent(operator):\n",
    "    \"\"\"\n",
    "    Returns a string representing the agent completing some action\n",
    "    Example: get_operation_agent(<Op (robot-move robot1 sq2-0 sq1-0)>)\n",
    "            should return \"robot-1\".\n",
    "\n",
    "    @param operator:    A pyperplan Operator object\n",
    "\n",
    "    @return:            A string of the agent completing the operation \n",
    "    \"\"\"\n",
    "    return re.search(\"(?<=\\s)(.*?)(?=\\s)\", operator.name).group()\n",
    "\n",
    "def explain(agent):\n",
    "    \"\"\"\n",
    "    Returns an Operator (explain `AGENT`)\n",
    "    \"\"\"\n",
    "    return task.Operator('(explain ' + str(agent) + ')', frozenset(), frozenset(), frozenset())\n",
    "\n",
    "def calculate_plan_cost(plan):\n",
    "    \"\"\"Calculates the cost of a plan. In our case (because each action has a unit cost), \n",
    "    the plan cost is simply the length of the plan. \"\"\"\n",
    "    return len(plan)\n",
    "\n",
    "def make_fully_explainable_plan(optimal_plan, expected_plan):\n",
    "    \"\"\"\n",
    "    Makes a fully explainable plan that offers explanations when the expected action \n",
    "    differs from the action the robot thinks is best.\n",
    "\n",
    "    @param optimal_plan:    The list of actions in an optimal plan\n",
    "    @param expected_plan:   The list of actions in the human-expected plan\n",
    "\n",
    "    @return explained_plan: The list of actions in the fully explainable plan\n",
    "    \"\"\"\n",
    "    # Initialize explained plan to the empty list\n",
    "    explained_plan = []\n",
    "\n",
    "    # Iterate through index in optimal plan\n",
    "    for i in range(len(optimal_plan)):\n",
    "        # Get optimal and expected action at step i\n",
    "        optimal_action = optimal_plan[i]\n",
    "        expected_action = expected_plan[i]\n",
    "\n",
    "        # Get the agent performing the optimal action\n",
    "        agent = get_action_agent(optimal_action)\n",
    "\n",
    "        # If the optimal action is what you'd expect, \n",
    "        # append the optimal action to the explained plan\n",
    "        if optimal_action == expected_action:\n",
    "            explained_plan.append(optimal_action)\n",
    "        # Otherwise, try to find the index \"next matching\" plan action \n",
    "        else:\n",
    "            try: \n",
    "                # See if the optimal action's add effects appear in the expected plan\n",
    "                # If they do, remove all of the actions in the expected plan\n",
    "                matching_index = [x.add_effects for x in expected_plan].index(optimal_action.add_effects)\n",
    "                expected_plan = expected_plan[:i] + expected_plan[matching_index:]\n",
    "                explained_plan.append(optimal_action)\n",
    "            except:\n",
    "                explained_plan.append(optimal_action)\n",
    "                explained_plan.append(explain(agent))\n",
    "\n",
    "    # Return the explained plan\n",
    "    return explained_plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_domain_file = 'pddl/human-domain.pddl'\n",
    "human_problem_file = 'pddl/human-problem.pddl'\n",
    "\n",
    "robot_domain_file = 'pddl/robot-domain.pddl'\n",
    "robot_problem_file = 'pddl/robot-problem.pddl'\n",
    "\n",
    "expected_plan = run_pyperplan(human_domain_file, human_problem_file)\n",
    "optimal_plan = run_pyperplan(robot_domain_file, robot_problem_file)\n",
    "explainable_plan = make_fully_explainable_plan(optimal_plan, expected_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Meta-MDP\n",
    "\n",
    "As discussed in class, one way of selecting robot behaviors can be modeled as a \"Meta-MDP\". We'll use the [Python MDP Toolbox](https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html). **TODO: Describe the problem, classes, features, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the problem statement, we'll model the problem as an infinite horizon discounted MDP of the form $$M = \\langle S, A, P, C, \\gamma \\rangle$$\n",
    "\n",
    "### Problem Description\n",
    "Our **state space**, $S$, are the human's \"trust level.\" For this implementation, we'll have four trust levels, so $\\| S \\| = 4$. We associate each trust level with numerical values $T = \\begin{bmatrix} 0 & 0.3 & 0.6 & 1.0 \\end{bmatrix}$, which we will use to help us model the rest of the problem.\n",
    "\n",
    "Our **action space**, $A$, is simple--the robot may choose between its own _optimal plan_ or the human's fully explainable (or _expected_ plan). (Therefore, $\\|A \\| = 2$, and $A(0) = \\pi^\\textrm{opt}$ and $A(1) = \\pi^\\textrm{exp}$). \n",
    "\n",
    "The **explicability score**, $E(\\pi)$, is the negative of the cost difference between the current plan and the optimal plan in the robot model. For example, $E(\\pi^\\textrm{exp}) = - (\\textrm{cost}_{\\pi^{\\textrm{exp}}} - \\textrm{cost}_{\\pi^{\\textrm{opt}}})$.\n",
    "\n",
    "As described in the [Python MDP Toolbox documentation](https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html), our transition matrix $P$ should be a [numpy array](https://numpy.org/doc/stable/reference/generated/numpy.array.html) of size `(2, 4, 4)`. Therefore, `P[k][i][j]` represents the likelihood of transitioning from state $s_i$ to state $s_j$ with the action $a_k$. There are two cases to think about when defining our transition matrix:\n",
    "\n",
    "1. The **optimal plan**. Here, we need to consider three subcases, as our robot is following a plan with a non-perfect explicability score. The trust level may either **decrease**, **stay the same**, or **increase**. \n",
    "   - We model the likelihood that the trust level **decreases** as $P(s_i, a^\\pi, s_{i-1}) = \\omega(i) * (1 - E(\\pi))$. \n",
    "   - We model the likelihood that the trust level **stays the same** as $P(s_i, a^\\pi, s_{i}) = \\omega(i) * E(\\pi)$.\n",
    "   - We model the likelihood that the trust level **increases** as $P(s_i, a^\\pi, s_{i+1}) = (1 - \\omega(i))$\n",
    "2.  The **expected plan**. Here, trust increases to the next level in all but the maximum trust level (where it is expected to remain the same).\n",
    "\n",
    "Note that `P(0)` corresponds with the transition matrix of the optimal plan, and `P(1)` corresponds with the transition matrix of the expected plan.\n",
    "\n",
    "\n",
    "The **cost**, $C$, is modeled as a `numpy array` of size `(4, 2)`. $C(s_i, a^\\pi) = (1 - \\omega(i)) * C_e(\\pi)$, where $C_e(\\pi)$ is the cost of the fully explainable plan. Here, we'll assume that each action has unit cost, but this cost function could certainly get more complicated!\n",
    "\n",
    "The likelihood that the human chooses to observe at some trust level, $\\omega(i)$ is modeled as a Bernoulli distribution with probability of $(1 - T(i))$. Here, $\\omega$ should be a `numpy array` of size `(1, 4)`.\n",
    "\n",
    "Your task is to fill in the transition matrix `P` and the reward matrix `R` to model the problem as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four trust states\n",
    "def omega(T):\n",
    "    \"\"\"\n",
    "    Builds the omega matrix. Recall that w(i) = (1 - T(i))\n",
    "\n",
    "    @param  T:     The numerical values for trust.\n",
    "    \n",
    "    @return w:     An np.array of np.shape(T). Recall that w(i)\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.ones(np.shape(T)) - T\n",
    "\n",
    "def explicability_score(optimal_cost, expected_cost):\n",
    "    \"\"\"\n",
    "    Builds the explicability score E(pi). Recall that E(plan) = - (plan_cost -\n",
    "    optimal_plan)\n",
    "\n",
    "    @param  optimal_cost:   The cost of the optimal plan.\n",
    "    @param  expected_cost:  The cost of the fully explicable plan.expected_plan\n",
    "\n",
    "    @return E:             The explicability score given the optimal and explicable plans.\n",
    "    \"\"\"\n",
    "    E = np.array([-(optimal_cost - optimal_cost),\n",
    "    - (expected_cost - optimal_cost)])\n",
    "    return E\n",
    "\n",
    "def transition_matrix(T, w, E):\n",
    "    \"\"\"\n",
    "    Builds the transition matrix, P. Recall that we have 2 actions (size of E)\n",
    "    and 4 states (size of T). Therefore, our P matrix is an np.array of size\n",
    "    (length(E), length(T), length(T).\n",
    "\n",
    "\n",
    "    The first action in P is following the optimal plan. The second action in P\n",
    "    is following the explainable plan. See the problem statement for a full\n",
    "    description of our expected transitions.\n",
    "    \n",
    "    @param  T: the trust levels\n",
    "    @param  w: the likelihood that the human observes\n",
    "    @param  E: the plan explicability score\n",
    "\n",
    "    @return P: the transition matrix\n",
    "    \"\"\"\n",
    "    P = np.zeros((len(E), len(T), len(T)))\n",
    "\n",
    "    # following the perfectly explicable plan\n",
    "    for i in range(len(P[1])):\n",
    "        P[1][i][min(len(P[0]) - 1, i + 1)] = 1.0\n",
    "\n",
    "    for i in range(len(P[0])):\n",
    "        if i != 3:\n",
    "            P[0][i][i+1] = 1 - w[i] # trust decreases\n",
    "            P[0][i][i] = w[i] * E[0]\n",
    "        else:\n",
    "            P[0][i][i] = 1 - w[i]\n",
    "        P[0][i][max([0, i-1])] = w[i] * (1 - E[0])\n",
    "\n",
    "    return P\n",
    "\n",
    "def cost(w, E, expected_cost):\n",
    "    C = np.zeros((len(w), len(E)))\n",
    "\n",
    "    for i in range(len(C)):\n",
    "        C[i][0] = (1 - w[i]) * expected_cost\n",
    "        C[i][1] = (1 - w[i]) * expected_cost\n",
    "    \n",
    "    return C\n",
    "        \n",
    "\n",
    "def build_meta_MDP(T, optimal_plan, expected_plan, gamma):\n",
    "    \"\"\"\n",
    "    Builds the Meta MDP model given trust levels and an optimal and expected plan.\n",
    "\n",
    "    @param          T: The matrix associating trust level with values in the range [0, 1].\n",
    "    @param          optimal_plan: A list of actions corresponding with the optimal plan.\n",
    "    @param          expected_plan: A list of actions corresponding with the explicable/expected plan.\n",
    "\n",
    "    @return P:      The transition matrix.\n",
    "    @return C:      The cost matrix.\n",
    "    @return gamma:  The discount factor\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the w (omega) matrix, representing the likelihood that a human will\n",
    "    # choose to observe\n",
    "    w = omega(T)\n",
    "    \n",
    "    # Get optimal and expected costs\n",
    "    optimal_cost = calculate_plan_cost(optimal_plan)\n",
    "    expected_cost = calculate_plan_cost(expected_plan)\n",
    "\n",
    "    # Get E (explicability score)\n",
    "    E = explicability_score(optimal_cost, expected_cost)\n",
    "\n",
    "    # Get P (transition matrix)\n",
    "    P = transition_matrix(T, w, E)\n",
    "\n",
    "    # Get C (cost)\n",
    "    C = cost(w, E, expected_cost)\n",
    "        \n",
    "    return P, C, gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll try running the MDP that we wrote! Execute the cell below to see your generated policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.array([0, 0.3, 0.6, 1])\n",
    "P, C, gamma = build_meta_MDP(T, optimal_plan, expected_plan, 0.9)\n",
    "\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, C, gamma)\n",
    "pi.run()\n",
    "\n",
    "pi.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that your policy takes action $1$ (the explainable plan) in all cases except where human trust is very high. This is what we might expect intuitively, so that's pretty cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2efb303a8c67ecba5167d2efb1d6294a5dab7ce44c08b1fa1f624f44f77a78fb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
